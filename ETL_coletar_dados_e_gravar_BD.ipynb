{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renanTrelarMadeiras/Receita_Federal_do_Brasil_-_Dados_Publicos_CNPJ/blob/master/ETL_coletar_dados_e_gravar_BD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMe42y4cWJd-"
      },
      "source": [
        "# 0. configurando ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INIT"
      ],
      "metadata": {
        "id": "ELpJdDiPy5La"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5gbrcqoWJeL",
        "outputId": "ea6747c9-997f-4325-981d-507e8160d6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 2)) (4.11.2)\n",
            "Requirement already satisfied: bs4>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 3)) (0.0.1)\n",
            "Requirement already satisfied: greenlet>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 5)) (7.0.0)\n",
            "Requirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 6)) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 7)) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 8)) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 10)) (1.0.0)\n",
            "Requirement already satisfied: pytz>=2021.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 11)) (2023.3.post1)\n",
            "Requirement already satisfied: requests==2.30.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 12)) (2.30.0)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 13)) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 14)) (2.5)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.18 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 15)) (2.0.23)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 16)) (4.5.0)\n",
            "Requirement already satisfied: tzdata==2023.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 17)) (2023.3)\n",
            "Requirement already satisfied: urllib3==2.0.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 18)) (2.0.2)\n",
            "Requirement already satisfied: wget>=3.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 19)) (3.2)\n",
            "Requirement already satisfied: zipp>=3.4.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 20)) (3.17.0)\n",
            "Requirement already satisfied: mysql-connector-python in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 24)) (8.2.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from -r /content/requeriment.txt (line 25)) (10.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.30.0->-r /content/requeriment.txt (line 12)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.30.0->-r /content/requeriment.txt (line 12)) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.30.0->-r /content/requeriment.txt (line 12)) (2023.11.17)\n",
            "Requirement already satisfied: protobuf<=4.21.12,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mysql-connector-python->-r /content/requeriment.txt (line 24)) (4.21.12)\n"
          ]
        }
      ],
      "source": [
        "%pip install -r \"/content/requeriment.txt\"\n",
        "# %pip install mysql-connector-python pandas pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V02qhjMWJeP"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "from dotenv import load_dotenv\n",
        "from sqlalchemy import create_engine\n",
        "import bs4 as bs\n",
        "import ftplib\n",
        "import gzip\n",
        "import os\n",
        "import pandas as pd\n",
        "# import psycopg2\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import requests\n",
        "import urllib.request\n",
        "import wget\n",
        "import zipfile\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qxkAAYRWJeV"
      },
      "source": [
        "Ler arquivo de configuração de ambiente # https://dev.to/jakewitcher/using-env-files-for-environment-variables-in-python-applications-55a1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEFS"
      ],
      "metadata": {
        "id": "1fHVqlA0y8Ne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxjfjXolWJea",
        "outputId": "2f2d8291-92ee-475c-8130-d200e63ba205"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# print('Especifique o local do seu arquivo de configuração \".env\". Por exemplo: C:\\...\\Receita_Federal_do_Brasil_-_Dados_Publicos_CNPJ\\code')\n",
        "# C:\\Aphonso_C\\Git\\Receita_Federal_do_Brasil_-_Dados_Publicos_CNPJ\\code\n",
        "# local_env = input()\n",
        "local_env = \"./\"\n",
        "# dotenv_path = os.path.join(local_env, '.env_template')\n",
        "dotenv_path = os.path.join(local_env, 'env')\n",
        "load_dotenv(dotenv_path=dotenv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzEz3dHCWJeZ"
      },
      "outputs": [],
      "source": [
        "def getEnv(env):\n",
        "    return os.getenv(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnwY1LeoWJeQ"
      },
      "outputs": [],
      "source": [
        "def check_diff(url, file_name):\n",
        "    '''\n",
        "    Verifica se o arquivo no servidor existe no disco e se ele tem o mesmo\n",
        "    tamanho no servidor.\n",
        "    '''\n",
        "    if not os.path.isfile(file_name):\n",
        "        return True # ainda nao foi baixado\n",
        "    response = requests.head(url)\n",
        "    new_size = int(response.headers.get('content-length', 0))\n",
        "    old_size = os.path.getsize(file_name)\n",
        "    if new_size != old_size:\n",
        "        os.remove(file_name)\n",
        "        return True # tamanho diferentes\n",
        "    return False # arquivos sao iguais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG2ZFdnZWJeS"
      },
      "outputs": [],
      "source": [
        "def makedirs(path):\n",
        "    '''\n",
        "    cria path caso seja necessario\n",
        "    '''\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y-kjSKrWJeT"
      },
      "outputs": [],
      "source": [
        "def to_sql(dataframe, **kwargs):\n",
        "    '''\n",
        "    Quebra em pedacos a tarefa de inserir registros no banco\n",
        "    '''\n",
        "    size = 4096\n",
        "    total = len(dataframe)\n",
        "    name = kwargs.get('name')\n",
        "    def chunker(df):\n",
        "        return (df[i:i + size] for i in range(0, len(df), size))\n",
        "    for i, df in enumerate(chunker(dataframe)):\n",
        "        df.to_sql(**kwargs)\n",
        "        index = i * size\n",
        "        percent = (index * 100) / total\n",
        "        progress = f'{name} {percent:.2f}% {index:0{len(str(total))}}/{total}'\n",
        "        sys.stdout.write(f'\\r{progress}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmBuGK9zWJee"
      },
      "outputs": [],
      "source": [
        "dados_rf = 'http://200.152.38.155/CNPJ/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuKyYblkWJef"
      },
      "source": [
        "Read details from \".env\" file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y4KlUivWJef",
        "outputId": "270cf63f-c525-4eda-ab37-b521e8ebe7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretórios definidos: \n",
            "output_files: ./OUTPUT_FILES/\n",
            "parquet_files: ./PATH_PARQUET\n",
            "extracted_files: ./EXTRACTED_FILES/\n"
          ]
        }
      ],
      "source": [
        "output_files = None\n",
        "extracted_files = None\n",
        "parquet_files = None\n",
        "try:\n",
        "    output_files = getEnv('OUTPUT_FILES_PATH')\n",
        "    makedirs(output_files)\n",
        "    extracted_files = getEnv('EXTRACTED_FILES_PATH')\n",
        "    makedirs(extracted_files)\n",
        "\n",
        "    parquet_files = getEnv('PATH_PARQUET')\n",
        "    makedirs(parquet_files)\n",
        "\n",
        "    print('Diretórios definidos: \\n' +\n",
        "          'output_files: ' + str(output_files)  + '\\n' +\n",
        "          'parquet_files: ' + str(parquet_files)  + '\\n' +\n",
        "          'extracted_files: ' + str(extracted_files)\n",
        "          )\n",
        "except:\n",
        "    pass\n",
        "    print('Erro na definição dos diretórios, verifique o arquivo \".env\" ou o local informado do seu arquivo de configuração.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIf1nTS6WJeh"
      },
      "outputs": [],
      "source": [
        "import pyarrow.parquet as pq\n",
        "\n",
        "# def to_parquet(dataframe, file_name):\n",
        "#     root = f\"./{parquet_files}/\"\n",
        "#     dataframe.to_parquet(f'{root}{file_name}.parquet', index=False, compression='snappy')\n",
        "\n",
        "def to_parquet(dataframe, file_name, append=True):\n",
        "    root = f\"./{parquet_files}/\"\n",
        "    file_path = f'{root}{file_name}.parquet'\n",
        "\n",
        "    if append and os.path.exists(file_path):\n",
        "        # Se o arquivo Parquet existir e a opção de append estiver ativada, acrescente os novos dados\n",
        "        existing_data = pd.read_parquet(file_path)\n",
        "        combined_data = pd.concat([existing_data, dataframe], ignore_index=True)\n",
        "        combined_data.to_parquet(file_path, index=False, compression='snappy',engine=\"pyarrow\")\n",
        "        # print(f'Dados acrescentados a {file_path}')\n",
        "    else:\n",
        "        # Se o arquivo Parquet não existir ou a opção de append estiver desativada, crie um novo\n",
        "        dataframe.to_parquet(file_path, index=False, compression='snappy',engine=\"pyarrow\")\n",
        "        # print(f'Novo arquivo Parquet criado em {file_path}')\n",
        "\n",
        "# def to_parquet(dataframe, file_name):\n",
        "#     root = f\"./{parquet_files}/\"\n",
        "#     dataframe.to_parquet(f'{root}{file_name}.parquet', index=False, compression='snappy')\n",
        "\n",
        "# def concat_parquet(file_name, file_name_2, final_file):\n",
        "#     root = f\"./{parquet_files}/\"\n",
        "\n",
        "#     existing_data = pd.read_parquet(root+file_name)\n",
        "#     existing_data = pd.read_parquet(root+file_name_2)\n",
        "#     combined_data = pd.concat([file_name, file_name_2], ignore_index=True)\n",
        "\n",
        "#     combined_data.to_parquet(f\"{root}+{final_file}.parquet\", index=False, compression='brotli',engine=\"pyarrow\")\n",
        "\n",
        "def concat_parquet(conjunto):\n",
        "    # import os\n",
        "\n",
        "    root = f\"./{parquet_files}/\"\n",
        "    # files = [name for name in os.listdir(root) if name.endswith('.parquet') and name.find(conjunto) ]\n",
        "    files = [name for name in os.listdir(root) if name.find(conjunto) > -1 ]\n",
        "    # files = [file_name,file_name_2]\n",
        "\n",
        "    try:\n",
        "      schema = pq.ParquetFile(root + files[0]).schema_arrow\n",
        "      # print(root + files[0], files)\n",
        "      # print(files[0])\n",
        "      print(files)\n",
        "      # print(schema)\n",
        "\n",
        "      # with pq.ParquetWriter(root + conjunto + '_geral.parquet', schema=schema) as writer:\n",
        "      #     for file in files:\n",
        "      #         writer.write_table(pq.read_table(root + file, schema=schema))\n",
        "    except :\n",
        "      print(f\"erro ao encontrar arquivos {conjunto}\")\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMiYXJwrWJei"
      },
      "source": [
        "# 1. lendo urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wkwf9gbUWJej"
      },
      "outputs": [],
      "source": [
        "raw_html = urllib.request.urlopen(dados_rf)\n",
        "raw_html = raw_html.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxufaBV5WJek"
      },
      "source": [
        "Formatar página e converter em string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJJv26w2WJek"
      },
      "outputs": [],
      "source": [
        "page_items = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "html_str = str(page_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u4-joO7WJel"
      },
      "source": [
        "# 2. Coletar arquivos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0om5nL_MWJel"
      },
      "source": [
        "## 2.1 Obter arquivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJgIfu-bWJem"
      },
      "outputs": [],
      "source": [
        "Files = []\n",
        "text = '.zip'\n",
        "for m in re.finditer(text, html_str):\n",
        "    i_start = m.start()-40\n",
        "    i_end = m.end()\n",
        "    i_loc = html_str[i_start:i_end].find('href=')+6\n",
        "    Files.append(html_str[i_start+i_loc:i_end])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U8livcGWJen"
      },
      "source": [
        "Correcao do nome dos arquivos devido a mudanca na estrutura do HTML da pagina - 31/07/22 - Aphonso Rafael"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1UoRm4lWJeo"
      },
      "outputs": [],
      "source": [
        "Files_clean = []\n",
        "for i in range(len(Files)):\n",
        "    if not Files[i].find('.zip\">') > -1:\n",
        "        Files_clean.append(Files[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0Iik_dYWJeo"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del Files\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHgxeE4eWJeo"
      },
      "outputs": [],
      "source": [
        "Files = Files_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9CiNwVTWJep",
        "outputId": "c09b51ed-66d8-425a-fe0e-bd79a095e1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivos que serão baixados:\n",
            "1 - Cnaes.zip\n",
            "2 - Empresas0.zip\n",
            "3 - Empresas1.zip\n",
            "4 - Empresas2.zip\n",
            "5 - Empresas3.zip\n",
            "6 - Empresas4.zip\n",
            "7 - Empresas5.zip\n",
            "8 - Empresas6.zip\n",
            "9 - Empresas7.zip\n",
            "10 - Empresas8.zip\n",
            "11 - Empresas9.zip\n",
            "12 - Estabelecimentos0.zip\n",
            "13 - Estabelecimentos1.zip\n",
            "14 - Estabelecimentos2.zip\n",
            "15 - Estabelecimentos3.zip\n",
            "16 - Estabelecimentos4.zip\n",
            "17 - Estabelecimentos5.zip\n",
            "18 - Estabelecimentos6.zip\n",
            "19 - Estabelecimentos7.zip\n",
            "20 - Estabelecimentos8.zip\n",
            "21 - Estabelecimentos9.zip\n",
            "22 - Motivos.zip\n",
            "23 - Municipios.zip\n",
            "24 - Naturezas.zip\n",
            "25 - Paises.zip\n",
            "26 - Qualificacoes.zip\n",
            "27 - Simples.zip\n",
            "28 - Socios0.zip\n",
            "29 - Socios1.zip\n",
            "30 - Socios2.zip\n",
            "31 - Socios3.zip\n",
            "32 - Socios4.zip\n",
            "33 - Socios5.zip\n",
            "34 - Socios6.zip\n",
            "35 - Socios7.zip\n",
            "36 - Socios8.zip\n",
            "37 - Socios9.zip\n"
          ]
        }
      ],
      "source": [
        "print('Arquivos que serão baixados:')\n",
        "i_f = 0\n",
        "for f in Files:\n",
        "    i_f += 1\n",
        "    print(str(i_f) + ' - ' + f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLn3j-xnWJep"
      },
      "source": [
        "## 2.2 DOWNLOAD :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-NfSL-FWJeq"
      },
      "source": [
        "####################\n",
        " DOWNLOAD #################### <br>\n",
        "Create this bar_progress method which is invoked automatically from wget:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k96Xv9P_WJeq"
      },
      "outputs": [],
      "source": [
        "def bar_progress(current, total, width=80, arquivo=''):\n",
        "  progress_message = \"Downloading: %d%% [%d / %d] bytes - \" % (current / total * 100, current, total)\n",
        "  # Don't use print() as it will print in new line every time.\n",
        "  sys.stdout.write(f\"\\r {arquivo}: {progress_message}\")\n",
        "  sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNm7tZ7yWJer"
      },
      "source": [
        "#################### Download arquivos ####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti3wb-uhWJes"
      },
      "outputs": [],
      "source": [
        "# paralelizando download de arquivos\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def download_file(url, output_files):\n",
        "    nome_arquivo = os.path.join(output_files, os.path.basename(url))\n",
        "    if check_diff(url, nome_arquivo):\n",
        "        wget.download(url, out=output_files, bar=bar_progress)\n",
        "\n",
        "# Número de threads para usar no download paralelo\n",
        "num_threads = 4 # Ajuste conforme necessário\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    # Use o executor para enviar tarefas de download em paralelo\n",
        "    tarefas_download = [executor.submit(download_file, dados_rf + l, output_files) for l in Files]\n",
        "\n",
        "    # Aguarde a conclusão de todas as tarefas\n",
        "    for futuro in tarefas_download:\n",
        "        futuro.result()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Lbf6yxWJet"
      },
      "source": [
        "Download layout:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0MvAJfvMWJet",
        "outputId": "b32589f3-ce1d-426c-c079-64a72f35f643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baixando layout:\n",
            " : Downloading: 100% [54000 / 54000] bytes - "
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./OUTPUT_FILES//NOVOLAYOUTDOSDADOSABERTOSDOCNPJ.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "Layout = 'https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/arquivos/NOVOLAYOUTDOSDADOSABERTOSDOCNPJ.pdf'\n",
        "print('Baixando layout:')\n",
        "wget.download(Layout, out=output_files, bar=bar_progress)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7riQkOTWJeu"
      },
      "source": [
        "## 2.3 Extraindo Arquivos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "DxS5U-b3WJeu",
        "outputId": "6e2ce871-07a2-4a23-c1e0-d592facde961"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-7817ffe4a2e9>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfuturo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarefas_extracao\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mfuturo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-7817ffe4a2e9>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m  \u001b[0;31m# Ajuste conforme necessário\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_threads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Use o executor para enviar tarefas de extração em paralelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtarefas_extracao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_zip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# paralelizar extração de arquivos zip\n",
        "def extract_zip(file_path, extracted_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extracted_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao extrair {file_path}: {e}\")\n",
        "\n",
        "# Número de threads para usar na extração paralela\n",
        "num_threads = 5  # Ajuste conforme necessário\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    # Use o executor para enviar tarefas de extração em paralelo\n",
        "    tarefas_extracao = [executor.submit(extract_zip, os.path.join(output_files, l), extracted_files) for l in Files]\n",
        "\n",
        "    # Aguarde a conclusão de todas as tarefas\n",
        "    for futuro in tarefas_extracao:\n",
        "        futuro.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsOlwi6OWJev"
      },
      "source": [
        "# 3. Ler e inserir dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_r4dKb_WJew"
      },
      "source": [
        "######################################################################################################################<br>\n",
        " LER E INSERIR DADOS #################################################################################################<br>\n",
        "######################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 carregar lista de dados locais"
      ],
      "metadata": {
        "id": "SW0d5We7zzqi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCV3d_avWJew"
      },
      "outputs": [],
      "source": [
        "insert_start = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsprkuffWJex"
      },
      "source": [
        "Files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-B97Ds6WJfB",
        "outputId": "03449a49-2f04-403a-9ddf-422e6fec4249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./EXTRACTED_FILES/\n"
          ]
        }
      ],
      "source": [
        "Items = [name for name in os.listdir(extracted_files) if name.endswith('')]\n",
        "\n",
        "print(extracted_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ph_yYAuWJfC"
      },
      "source": [
        "Separar arquivos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5m9dvrmWJfC"
      },
      "outputs": [],
      "source": [
        "arquivos_empresa = []\n",
        "arquivos_estabelecimento = []\n",
        "arquivos_socios = []\n",
        "arquivos_simples = []\n",
        "arquivos_cnae = []\n",
        "arquivos_moti = []\n",
        "arquivos_munic = []\n",
        "arquivos_natju = []\n",
        "arquivos_pais = []\n",
        "arquivos_quals = []\n",
        "for i in range(len(Items)):\n",
        "    if Items[i].find('EMPRE') > -1:\n",
        "        arquivos_empresa.append(Items[i])\n",
        "    elif Items[i].find('ESTABELE') > -1:\n",
        "        arquivos_estabelecimento.append(Items[i])\n",
        "    elif Items[i].find('SOCIO') > -1:\n",
        "        arquivos_socios.append(Items[i])\n",
        "    elif Items[i].find('SIMPLES') > -1:\n",
        "        arquivos_simples.append(Items[i])\n",
        "    elif Items[i].find('CNAE') > -1:\n",
        "        arquivos_cnae.append(Items[i])\n",
        "    elif Items[i].find('MOTI') > -1:\n",
        "        arquivos_moti.append(Items[i])\n",
        "    elif Items[i].find('MUNIC') > -1:\n",
        "        arquivos_munic.append(Items[i])\n",
        "    elif Items[i].find('NATJU') > -1:\n",
        "        arquivos_natju.append(Items[i])\n",
        "    elif Items[i].find('PAIS') > -1:\n",
        "        arquivos_pais.append(Items[i])\n",
        "    elif Items[i].find('QUALS') > -1:\n",
        "        arquivos_quals.append(Items[i])\n",
        "    else:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_dm2NwhWJfE"
      },
      "source": [
        "Conectar no banco de dados:<br>\n",
        "Dados da conexão com o BD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzoUXkf8WJfF"
      },
      "outputs": [],
      "source": [
        "user=getEnv('DB_USER')\n",
        "passw=getEnv('DB_PASSWORD')\n",
        "host=getEnv('DB_HOST')\n",
        "port=getEnv('DB_PORT')\n",
        "database=getEnv('DB_NAME')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn6SPHk_WJfF"
      },
      "source": [
        "Conectar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wr9bEVTWJfF"
      },
      "outputs": [],
      "source": [
        "# conector para mysql\n",
        "# engine = create_engine(f'mysql+mysqlconnector://{user}:{passw}@{host}:{port}/{database}')\n",
        "# cur = conn.cursor()\n",
        "\n",
        "# engine = create_engine('postgresql://'+user+':'+passw+'@'+host+':'+port+'/'+database)\n",
        "# conn = psycopg2.connect('dbname='+database+' '+'user='+user+' '+'host='+host+' '+'port='+port+' '+'password='+passw)\n",
        "# cur = conn.cursor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWrPNYk5WJfG"
      },
      "source": [
        "## 3.1 Dados Empresas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sHGqdhaWJfG",
        "outputId": "003b3590-e11e-44ba-bb6f-520d34574954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#######################\n",
            "## Arquivos de EMPRESA:\n",
            "#######################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "empresa_insert_start = time.time()\n",
        "# print(\"\"\"\n",
        "# #######################\n",
        "# ## Arquivos de EMPRESA:\n",
        "# #######################\n",
        "# \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heGwXLuNWJfG"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv_LY_wPWJfJ",
        "outputId": "98d7cbff-11e8-4c6f-a15d-fe496e9fbfab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trabalhando no arquivo: K3241.K03200Y6.D31209.EMPRECSV [...]\n",
            "Novo arquivo Parquet criado em ././PATH_PARQUET/empresa_K3241.K03200Y6.D31209.EMPRECSV.parquet\n",
            "\t Arquivo K3241.K03200Y6.D31209.EMPRECSV inserido com sucesso no banco de dados!\n",
            "Trabalhando no arquivo: K3241.K03200Y2.D31209.EMPRECSV [...]\n",
            "Novo arquivo Parquet criado em ././PATH_PARQUET/empresa_K3241.K03200Y2.D31209.EMPRECSV.parquet\n",
            "\t Arquivo K3241.K03200Y2.D31209.EMPRECSV inserido com sucesso no banco de dados!\n",
            "Trabalhando no arquivo: K3241.K03200Y1.D31209.EMPRECSV [...]\n",
            "Novo arquivo Parquet criado em ././PATH_PARQUET/empresa_K3241.K03200Y1.D31209.EMPRECSV.parquet\n",
            "\t Arquivo K3241.K03200Y1.D31209.EMPRECSV inserido com sucesso no banco de dados!\n",
            "Trabalhando no arquivo: K3241.K03200Y0.D31209.EMPRECSV [...]\n",
            "Novo arquivo Parquet criado em ././PATH_PARQUET/empresa_K3241.K03200Y0.D31209.EMPRECSV.parquet\n",
            "\t Arquivo K3241.K03200Y0.D31209.EMPRECSV inserido com sucesso no banco de dados!\n",
            "Trabalhando no arquivo: K3241.K03200Y7.D31209.EMPRECSV [...]\n",
            "Novo arquivo Parquet criado em ././PATH_PARQUET/empresa_K3241.K03200Y7.D31209.EMPRECSV.parquet\n",
            "\t Arquivo K3241.K03200Y7.D31209.EMPRECSV inserido com sucesso no banco de dados!\n",
            "Trabalhando no arquivo: K3241.K03200Y5.D31209.EMPRECSV [...]\n",
            "Novo arquivo Parquet criado em ././PATH_PARQUET/empresa_K3241.K03200Y5.D31209.EMPRECSV.parquet\n",
            "\t Arquivo K3241.K03200Y5.D31209.EMPRECSV inserido com sucesso no banco de dados!\n",
            "Trabalhando no arquivo: K3241.K03200Y4.D31209.EMPRECSV [...]\n",
            "Novo arquivo Parquet criado em ././PATH_PARQUET/empresa_K3241.K03200Y4.D31209.EMPRECSV.parquet\n",
            "\t Arquivo K3241.K03200Y4.D31209.EMPRECSV inserido com sucesso no banco de dados!\n",
            "Trabalhando no arquivo: K3241.K03200Y8.D31209.EMPRECSV [...]\n",
            "Novo arquivo Parquet criado em ././PATH_PARQUET/empresa_K3241.K03200Y8.D31209.EMPRECSV.parquet\n",
            "\t Arquivo K3241.K03200Y8.D31209.EMPRECSV inserido com sucesso no banco de dados!\n",
            "Trabalhando no arquivo: K3241.K03200Y9.D31209.EMPRECSV [...]\n",
            "Novo arquivo Parquet criado em ././PATH_PARQUET/empresa_K3241.K03200Y9.D31209.EMPRECSV.parquet\n",
            "\t Arquivo K3241.K03200Y9.D31209.EMPRECSV inserido com sucesso no banco de dados!\n",
            "Trabalhando no arquivo: K3241.K03200Y3.D31209.EMPRECSV [...]\n",
            "Novo arquivo Parquet criado em ././PATH_PARQUET/empresa_K3241.K03200Y3.D31209.EMPRECSV.parquet\n",
            "\t Arquivo K3241.K03200Y3.D31209.EMPRECSV inserido com sucesso no banco de dados!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for e in range(0, len(arquivos_empresa)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_empresa[e]+' [...]')\n",
        "    try:\n",
        "        del empresa\n",
        "    except:\n",
        "        pass\n",
        "    empresa = pd.DataFrame(columns=[0, 1, 2, 3, 4, 5, 6])\n",
        "    empresa_dtypes = {0: 'object', 1: 'object', 2: 'object', 3: 'object', 4: 'object', 5: 'object', 6: 'object'}\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_empresa[e])\n",
        "    empresa = pd.read_csv(filepath_or_buffer=extracted_file_path,\n",
        "                          sep=';',\n",
        "                          #nrows=100,\n",
        "                          skiprows=0,\n",
        "                          header=None,\n",
        "                          dtype=empresa_dtypes,\n",
        "                          encoding='latin-1'\n",
        "    )\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    empresa = empresa.reset_index()\n",
        "    del empresa['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    empresa.columns = ['cnpj_basico', 'razao_social', 'natureza_juridica', 'qualificacao_responsavel', 'capital_social', 'porte_empresa', 'ente_federativo_responsavel']\n",
        "\n",
        "    # Replace \",\" por \".\"\n",
        "    empresa['capital_social'] = empresa['capital_social'].apply(lambda x: x.replace(',','.'))\n",
        "    empresa['capital_social'] = empresa['capital_social'].astype(float)\n",
        "\n",
        "    to_parquet(empresa,f'empresa_{arquivos_empresa[e]}')\n",
        "\n",
        "    # to_sql(empresa, name='empresa', con=engine, if_exists='append', index=False)\n",
        "    print('\\t Arquivo ' + arquivos_empresa[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Items = [name for name in os.listdir(parquet_files) if name.endswith('.parquet') and name.find(\"EMPRE\") ]\n",
        "concat_parquet(\"EMPRE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAO3wnLzBhU7",
        "outputId": "103a325e-012e-461a-f256-e4581cb9162c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "empresa_K3241.K03200Y6.D31209.EMPRECSV.parquet\n",
            "['empresa_K3241.K03200Y6.D31209.EMPRECSV.parquet', 'empresa_K3241.K03200Y8.D31209.EMPRECSV.parquet', 'empresa_K3241.K03200Y3.D31209.EMPRECSV.parquet', 'empresa_K3241.K03200Y5.D31209.EMPRECSV.parquet', 'empresa_K3241.K03200Y9.D31209.EMPRECSV.parquet', 'empresa_K3241.K03200Y2.D31209.EMPRECSV.parquet', 'empresa_K3241.K03200Y7.D31209.EMPRECSV.parquet', 'empresa_K3241.K03200Y0.D31209.EMPRECSV.parquet', 'empresa_K3241.K03200Y4.D31209.EMPRECSV.parquet', 'empresa_K3241.K03200Y1.D31209.EMPRECSV.parquet']\n",
            "cnpj_basico: string\n",
            "razao_social: string\n",
            "natureza_juridica: string\n",
            "qualificacao_responsavel: string\n",
            "capital_social: double\n",
            "porte_empresa: string\n",
            "ente_federativo_responsavel: string\n",
            "-- schema metadata --\n",
            "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 1012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "gESOk3FQWJfK",
        "outputId": "d0388a8f-d03e-420a-e997-6986969af8ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivos de empresa finalizados!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-341124dd050d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Arquivos de empresa finalizados!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mempresa_insert_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mempresa_Tempo_insert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempresa_insert_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mempresa_insert_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tempo de execução do processo de empresa (em segundos): '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempresa_Tempo_insert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'empresa_insert_start' is not defined"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    del empresa\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de empresa finalizados!')\n",
        "empresa_insert_end = time.time()\n",
        "empresa_Tempo_insert = round((empresa_insert_end - empresa_insert_start))\n",
        "print('Tempo de execução do processo de empresa (em segundos): ' + str(empresa_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BFB_JFZWJfK"
      },
      "source": [
        "## 3.2 Arquivos de estabelecimento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfxZ58UhWJfL"
      },
      "outputs": [],
      "source": [
        "estabelecimento_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "###############################\n",
        "## Arquivos de ESTABELECIMENTO:\n",
        "###############################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJYLv82xWJfM"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiFM_zXtWJfM"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"estabelecimento\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "i5ttRcVlWJfM",
        "outputId": "3d08ccea-5e98-40a7-db08-c191a23d9a66"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    concat_parquet(\"ESTABELE\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "for e in range(0, len(arquivos_estabelecimento)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_estabelecimento[e]+' [...]')\n",
        "    try:\n",
        "        del estabelecimento\n",
        "    except:\n",
        "        pass\n",
        "    estabelecimento = pd.DataFrame(columns=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28])\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_estabelecimento[e])\n",
        "    estabelecimento = pd.read_csv(filepath_or_buffer=extracted_file_path,\n",
        "                          sep=';',\n",
        "                          #nrows=100,\n",
        "                          skiprows=0,\n",
        "                          header=None,\n",
        "                          dtype='object',\n",
        "                          encoding='latin-1',\n",
        "    )\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    estabelecimento = estabelecimento.reset_index()\n",
        "    del estabelecimento['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    estabelecimento.columns = ['cnpj_basico',\n",
        "                               'cnpj_ordem',\n",
        "                               'cnpj_dv',\n",
        "                               'identificador_matriz_filial',\n",
        "                               'nome_fantasia',\n",
        "                               'situacao_cadastral',\n",
        "                               'data_situacao_cadastral',\n",
        "                               'motivo_situacao_cadastral',\n",
        "                               'nome_cidade_exterior',\n",
        "                               'pais',\n",
        "                               'data_inicio_atividade',\n",
        "                               'cnae_fiscal_principal',\n",
        "                               'cnae_fiscal_secundaria',\n",
        "                               'tipo_logradouro',\n",
        "                               'logradouro',\n",
        "                               'numero',\n",
        "                               'complemento',\n",
        "                               'bairro',\n",
        "                               'cep',\n",
        "                               'uf',\n",
        "                               'municipio',\n",
        "                               'ddd_1',\n",
        "                               'telefone_1',\n",
        "                               'ddd_2',\n",
        "                               'telefone_2',\n",
        "                               'ddd_fax',\n",
        "                               'fax',\n",
        "                               'correio_eletronico',\n",
        "                               'situacao_especial',\n",
        "                               'data_situacao_especial']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # estabelecimento\n",
        "    # to_sql(estabelecimento, name='estabelecimento', con=engine, if_exists='append', index=False)\n",
        "    to_sql(estabelecimento, name='estabelecimento', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_estabelecimento[e] + ' inserido com sucesso no banco de dados!')\n",
        "\n",
        "concat_parquet(\"ESTABELE\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5-5Wgjzsavj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5jcUQTuWJfN"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del estabelecimento\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de estabelecimento finalizados!')\n",
        "estabelecimento_insert_end = time.time()\n",
        "estabelecimento_Tempo_insert = round((estabelecimento_insert_end - estabelecimento_insert_start))\n",
        "print('Tempo de execução do processo de estabelecimento (em segundos): ' + str(estabelecimento_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdeHnxWCWJfO"
      },
      "source": [
        "## 3.3 Arquivos de socios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wclmoXLAWJfO"
      },
      "outputs": [],
      "source": [
        "socios_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "######################\n",
        "## Arquivos de SOCIOS:\n",
        "######################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfMIw2KCWJfO"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbmGXqeHWJfP"
      },
      "outputs": [],
      "source": [
        "cur.execute('DROP TABLE IF EXISTS \"socios\";')\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBmo9Xp4WJfP"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_socios)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_socios[e]+' [...]')\n",
        "    try:\n",
        "        del socios\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_socios[e])\n",
        "    socios = pd.DataFrame(columns=[1,2,3,4,5,6,7,8,9,10,11])\n",
        "    socios = pd.read_csv(filepath_or_buffer=extracted_file_path,\n",
        "                          sep=';',\n",
        "                          #nrows=100,\n",
        "                          skiprows=0,\n",
        "                          header=None,\n",
        "                          dtype='object',\n",
        "                          encoding='latin-1',\n",
        "    )\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    socios = socios.reset_index()\n",
        "    del socios['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    socios.columns = ['cnpj_basico',\n",
        "                      'identificador_socio',\n",
        "                      'nome_socio_razao_social',\n",
        "                      'cpf_cnpj_socio',\n",
        "                      'qualificacao_socio',\n",
        "                      'data_entrada_sociedade',\n",
        "                      'pais',\n",
        "                      'representante_legal',\n",
        "                      'nome_do_representante',\n",
        "                      'qualificacao_representante_legal',\n",
        "                      'faixa_etaria']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # socios\n",
        "    to_sql(socios, name='socios', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_socios[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTebXRPtWJfQ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del socios\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de socios finalizados!')\n",
        "socios_insert_end = time.time()\n",
        "socios_Tempo_insert = round((socios_insert_end - socios_insert_start))\n",
        "print('Tempo de execução do processo de sócios (em segundos): ' + str(socios_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwpiG7pWWJfQ"
      },
      "source": [
        "## 3.4 Arquivos de simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMJHwmxOWJfQ"
      },
      "outputs": [],
      "source": [
        "simples_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "################################\n",
        "## Arquivos do SIMPLES NACIONAL:\n",
        "################################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7dCZLq7WJfR"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26oE4jMQWJfR"
      },
      "outputs": [],
      "source": [
        "cur.execute('DROP TABLE IF EXISTS \"simples\";')\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vIiQTTKWJfS"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_simples)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_simples[e]+' [...]')\n",
        "    try:\n",
        "        del simples\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Verificar tamanho do arquivo:\n",
        "    print('Lendo o arquivo ' + arquivos_simples[e]+' [...]')\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_simples[e])\n",
        "    simples_lenght = sum(1 for line in open(extracted_file_path, \"r\"))\n",
        "    print('Linhas no arquivo do Simples '+ arquivos_simples[e] +': '+str(simples_lenght))\n",
        "    tamanho_das_partes = 1000000 # Registros por carga\n",
        "    partes = round(simples_lenght / tamanho_das_partes)\n",
        "    nrows = tamanho_das_partes\n",
        "    skiprows = 0\n",
        "    print('Este arquivo será dividido em ' + str(partes) + ' partes para inserção no banco de dados')\n",
        "    for i in range(0, partes):\n",
        "        print('Iniciando a parte ' + str(i+1) + ' [...]')\n",
        "        simples = pd.DataFrame(columns=[1,2,3,4,5,6])\n",
        "        simples = pd.read_csv(filepath_or_buffer=extracted_file_path,\n",
        "                              sep=';',\n",
        "                              nrows=nrows,\n",
        "                              skiprows=skiprows,\n",
        "                              header=None,\n",
        "                              dtype='object',\n",
        "                              encoding='latin-1',\n",
        "        )\n",
        "\n",
        "        # Tratamento do arquivo antes de inserir na base:\n",
        "        simples = simples.reset_index()\n",
        "        del simples['index']\n",
        "\n",
        "        # Renomear colunas\n",
        "        simples.columns = ['cnpj_basico',\n",
        "                           'opcao_pelo_simples',\n",
        "                           'data_opcao_simples',\n",
        "                           'data_exclusao_simples',\n",
        "                           'opcao_mei',\n",
        "                           'data_opcao_mei',\n",
        "                           'data_exclusao_mei']\n",
        "        skiprows = skiprows+nrows\n",
        "\n",
        "        # Gravar dados no banco:\n",
        "        # simples\n",
        "        to_sql(simples, name='simples', con=engine, if_exists='append', index=False)\n",
        "        print('Arquivo ' + arquivos_simples[e] + ' inserido com sucesso no banco de dados! - Parte '+ str(i+1))\n",
        "        try:\n",
        "            del simples\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "actvl9J-WJfS"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del simples\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzFL9IjYWJfT"
      },
      "outputs": [],
      "source": [
        "print('Arquivos do simples finalizados!')\n",
        "simples_insert_end = time.time()\n",
        "simples_Tempo_insert = round((simples_insert_end - simples_insert_start))\n",
        "print('Tempo de execução do processo do Simples Nacional (em segundos): ' + str(simples_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCtZW3BmWJfU"
      },
      "source": [
        "## 3.5 Arquivos de cnae:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWbUxFP8WJfU"
      },
      "outputs": [],
      "source": [
        "cnae_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "######################\n",
        "## Arquivos de cnae:\n",
        "######################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw9BJRKKWJfU"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbESGblrWJfV"
      },
      "outputs": [],
      "source": [
        "cur.execute('DROP TABLE IF EXISTS \"cnae\";')\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjrA1PqWWJfV"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_cnae)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_cnae[e]+' [...]')\n",
        "    try:\n",
        "        del cnae\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_cnae[e])\n",
        "    cnae = pd.DataFrame(columns=[1,2])\n",
        "    cnae = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    cnae = cnae.reset_index()\n",
        "    del cnae['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    cnae.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # cnae\n",
        "    to_sql(cnae, name='cnae', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_cnae[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzX8NT0tWJfW"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del cnae\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de cnae finalizados!')\n",
        "cnae_insert_end = time.time()\n",
        "cnae_Tempo_insert = round((cnae_insert_end - cnae_insert_start))\n",
        "print('Tempo de execução do processo de cnae (em segundos): ' + str(cnae_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KscfVG9CWJfW"
      },
      "source": [
        "## 3.6 Arquivos de moti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6DbuzQ5WJfW"
      },
      "outputs": [],
      "source": [
        "moti_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "#########################################\n",
        "## Arquivos de motivos da situação atual:\n",
        "#########################################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79QDQvlWJfX"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfC4_rDvWJfX"
      },
      "outputs": [],
      "source": [
        "cur.execute('DROP TABLE IF EXISTS \"moti\";')\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRLE8y1RWJfY"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_moti)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_moti[e]+' [...]')\n",
        "    try:\n",
        "        del moti\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_moti[e])\n",
        "    moti = pd.DataFrame(columns=[1,2])\n",
        "    moti = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    moti = moti.reset_index()\n",
        "    del moti['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    moti.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # moti\n",
        "    to_sql(moti, name='moti', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_moti[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoTgH0VFWJfZ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del moti\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de moti finalizados!')\n",
        "moti_insert_end = time.time()\n",
        "moti_Tempo_insert = round((moti_insert_end - moti_insert_start))\n",
        "print('Tempo de execução do processo de motivos da situação atual (em segundos): ' + str(moti_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW1Xod4lWJfa"
      },
      "source": [
        "## 3.7 Arquivos de munic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JoslmZoWJfb"
      },
      "outputs": [],
      "source": [
        "munic_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "##########################\n",
        "## Arquivos de municípios:\n",
        "##########################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7jwDQwHWJfc"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8RV6iPvWJfc"
      },
      "outputs": [],
      "source": [
        "cur.execute('DROP TABLE IF EXISTS \"munic\";')\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxuCbjHFWJfd"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_munic)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_munic[e]+' [...]')\n",
        "    try:\n",
        "        del munic\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_munic[e])\n",
        "    munic = pd.DataFrame(columns=[1,2])\n",
        "    munic = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    munic = munic.reset_index()\n",
        "    del munic['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    munic.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # munic\n",
        "    to_sql(munic, name='munic', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_munic[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN8ZfVenWJfd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del munic\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de munic finalizados!')\n",
        "munic_insert_end = time.time()\n",
        "munic_Tempo_insert = round((munic_insert_end - munic_insert_start))\n",
        "print('Tempo de execução do processo de municípios (em segundos): ' + str(munic_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpIEt6SXWJfd"
      },
      "source": [
        "## 3.8 Arquivos de natureza juridica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVhaLZbYWJfe"
      },
      "outputs": [],
      "source": [
        "natju_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "#################################\n",
        "## Arquivos de natureza jurídica:\n",
        "#################################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXALXdSEWJfe"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZn6aDEPWJfn"
      },
      "outputs": [],
      "source": [
        "cur.execute('DROP TABLE IF EXISTS \"natju\";')\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s-htYORWJfo"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_natju)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_natju[e]+' [...]')\n",
        "    try:\n",
        "        del natju\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_natju[e])\n",
        "    natju = pd.DataFrame(columns=[1,2])\n",
        "    natju = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    natju = natju.reset_index()\n",
        "    del natju['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    natju.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # natju\n",
        "    to_sql(natju, name='natju', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_natju[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QD4oc1wWJfp"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del natju\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de natju finalizados!')\n",
        "natju_insert_end = time.time()\n",
        "natju_Tempo_insert = round((natju_insert_end - natju_insert_start))\n",
        "print('Tempo de execução do processo de natureza jurídica (em segundos): ' + str(natju_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FRJ0nGoWJfq"
      },
      "source": [
        "## 3.9 Arquivos de pais:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCweX6QDWJfq"
      },
      "outputs": [],
      "source": [
        "pais_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "######################\n",
        "## Arquivos de país:\n",
        "######################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDfVhW4xWJfr"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVdDizxPWJfs"
      },
      "outputs": [],
      "source": [
        "cur.execute('DROP TABLE IF EXISTS \"pais\";')\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7YG0I36WJfs"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_pais)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_pais[e]+' [...]')\n",
        "    try:\n",
        "        del pais\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_pais[e])\n",
        "    pais = pd.DataFrame(columns=[1,2])\n",
        "    pais = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    pais = pais.reset_index()\n",
        "    del pais['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    pais.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # pais\n",
        "    to_sql(pais, name='pais', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_pais[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsu41onPWJfs"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del pais\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de pais finalizados!')\n",
        "pais_insert_end = time.time()\n",
        "pais_Tempo_insert = round((pais_insert_end - pais_insert_start))\n",
        "print('Tempo de execução do processo de país (em segundos): ' + str(pais_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mqsfJf1WJft"
      },
      "source": [
        "## 3.10 Arquivos de qualificação de sócios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAKyrhq_WJft"
      },
      "outputs": [],
      "source": [
        "quals_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "######################################\n",
        "## Arquivos de qualificação de sócios:\n",
        "######################################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTZmPQYlWJft"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnGzMZtrWJfu"
      },
      "outputs": [],
      "source": [
        "cur.execute('DROP TABLE IF EXISTS \"quals\";')\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJhAzMGUWJfu"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_quals)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_quals[e]+' [...]')\n",
        "    try:\n",
        "        del quals\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_quals[e])\n",
        "    quals = pd.DataFrame(columns=[1,2])\n",
        "    quals = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    quals = quals.reset_index()\n",
        "    del quals['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    quals.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # quals\n",
        "    to_sql(quals, name='quals', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_quals[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbSQQDR0WJfu"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del quals\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de quals finalizados!')\n",
        "quals_insert_end = time.time()\n",
        "quals_Tempo_insert = round((quals_insert_end - quals_insert_start))\n",
        "print('Tempo de execução do processo de qualificação de sócios (em segundos): ' + str(quals_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq2yKoBpWJfv"
      },
      "source": [
        "%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv66FfrbWJfv"
      },
      "outputs": [],
      "source": [
        "insert_end = time.time()\n",
        "Tempo_insert = round((insert_end - insert_start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfdDk5jGWJfv"
      },
      "source": [
        "# 4 Finalizando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoSMTdguWJfw"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "#############################################<br>\n",
        "## Processo de carga dos arquivos finalizado!<br>\n",
        "#############################################<br>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rENO86OiWJfw"
      },
      "outputs": [],
      "source": [
        "print('Tempo total de execução do processo de carga (em segundos): ' + str(Tempo_insert)) # Tempo de execução do processo (em segundos): 17.770 (4hrs e 57 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHnj4yLIWJfw"
      },
      "source": [
        "###############################<br>\n",
        "Tamanho dos arquivos:<br>\n",
        "empresa = 45.811.638<br>\n",
        "estabelecimento = 48.421.619<br>\n",
        "socios = 20.426.417<br>\n",
        "simples = 27.893.923<br>\n",
        "###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fstRNIRWJfx"
      },
      "source": [
        "%<br>\n",
        "Criar índices na base de dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRvyuLl_WJfx"
      },
      "outputs": [],
      "source": [
        "index_start = time.time()\n",
        "print(\"\"\"\n",
        "#######################################\n",
        "## Criar índices na base de dados [...]\n",
        "#######################################\n",
        "\"\"\")\n",
        "cur.execute(\"\"\"\n",
        "create index empresa_cnpj on empresa(cnpj_basico);\n",
        "commit;\n",
        "create index estabelecimento_cnpj on estabelecimento(cnpj_basico);\n",
        "commit;\n",
        "create index socios_cnpj on socios(cnpj_basico);\n",
        "commit;\n",
        "create index simples_cnpj on simples(cnpj_basico);\n",
        "commit;\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "print(\"\"\"\n",
        "############################################################\n",
        "## Índices criados nas tabelas, para a coluna `cnpj_basico`:\n",
        "   - empresa\n",
        "   - estabelecimento\n",
        "   - socios\n",
        "   - simples\n",
        "############################################################\n",
        "\"\"\")\n",
        "index_end = time.time()\n",
        "index_time = round(index_end - index_start)\n",
        "print('Tempo para criar os índices (em segundos): ' + str(index_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVgcD7gvWJfx"
      },
      "source": [
        "%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdYWHADCWJfy"
      },
      "source": [
        "print(\n",
        "Processo 100% finalizado! Você já pode usar seus dados no BD!<br>\n",
        " - Desenvolvido por: Aphonso Henrique do Amaral Rafael<br>\n",
        " - Contribua com esse projeto aqui: https://github.com/aphonsoar/Receita_Federal_do_Brasil_-_Dados_Publicos_CNPJ<br>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}