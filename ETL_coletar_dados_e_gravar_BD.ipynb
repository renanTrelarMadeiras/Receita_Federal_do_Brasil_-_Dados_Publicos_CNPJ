{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renanTrelarMadeiras/Receita_Federal_do_Brasil_-_Dados_Publicos_CNPJ/blob/master/ETL_coletar_dados_e_gravar_BD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1LcEV3DzCyx"
      },
      "source": [
        "# 0. configurando ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLgF3Ha15D2l"
      },
      "source": [
        "### arquivos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFXLnHkR5IVw"
      },
      "source": [
        "## script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e29mLgL7zCy0",
        "outputId": "adc5c586-a3af-4087-d8fa-f6d4260f415d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 3)) (4.12.3)\n",
            "Requirement already satisfied: bs4>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 4)) (0.0.2)\n",
            "Requirement already satisfied: greenlet>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 6)) (7.0.1)\n",
            "Requirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 7)) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 8)) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 9)) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: pytz>=2021.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 12)) (2023.4)\n",
            "Requirement already satisfied: requests==2.30.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 13)) (2.30.0)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 15)) (2.5)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.18 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 16)) (2.0.27)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 17)) (4.9.0)\n",
            "Requirement already satisfied: tzdata==2023.3 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 18)) (2023.3)\n",
            "Requirement already satisfied: urllib3==2.0.2 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 19)) (2.0.2)\n",
            "Requirement already satisfied: wget>=3.2 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 20)) (3.2)\n",
            "Requirement already satisfied: zipp>=3.4.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 21)) (3.17.0)\n",
            "Requirement already satisfied: mysql-connector-python in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 25)) (8.3.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 26)) (14.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.30.0->-r ./requirements.txt (line 13)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.30.0->-r ./requirements.txt (line 13)) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.30.0->-r ./requirements.txt (line 13)) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install -r \"./requirements.txt\"\n",
        "\n",
        "# %pip install mysql-connector-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8aMDRrjzCy1"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "from dotenv import load_dotenv\n",
        "from sqlalchemy import create_engine\n",
        "import bs4 as bs\n",
        "import ftplib\n",
        "import gzip\n",
        "import os\n",
        "import pandas as pd\n",
        "# import psycopg2\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import requests\n",
        "import urllib.request\n",
        "import wget\n",
        "import zipfile\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d96KDiW1zCy4"
      },
      "outputs": [],
      "source": [
        "def check_diff(url, file_name):\n",
        "    '''\n",
        "    Verifica se o arquivo no servidor existe no disco e se ele tem o mesmo\n",
        "    tamanho no servidor.\n",
        "    '''\n",
        "    if not os.path.isfile(file_name):\n",
        "        return True # ainda nao foi baixado\n",
        "    response = requests.head(url)\n",
        "    new_size = int(response.headers.get('content-length', 0))\n",
        "    old_size = os.path.getsize(file_name)\n",
        "    if new_size != old_size:\n",
        "        os.remove(file_name)\n",
        "        return True # tamanho diferentes\n",
        "    return False # arquivos sao iguais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRyOsjf4zCy6"
      },
      "outputs": [],
      "source": [
        "def makedirs(path):\n",
        "    '''\n",
        "    cria path caso seja necessario\n",
        "    '''\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9gRFAerzCy8"
      },
      "outputs": [],
      "source": [
        "def to_sql(dataframe, **kwargs):\n",
        "    '''\n",
        "    Quebra em pedacos a tarefa de inserir registros no banco\n",
        "    '''\n",
        "    size = 4096\n",
        "    total = len(dataframe)\n",
        "    name = kwargs.get('name')\n",
        "    def chunker(df):\n",
        "        return (df[i:i + size] for i in range(0, len(df), size))\n",
        "    for i, df in enumerate(chunker(dataframe)):\n",
        "        df.to_sql(**kwargs)\n",
        "        index = i * size\n",
        "        percent = (index * 100) / total\n",
        "        progress = f'{name} {percent:.2f}% {index:0{len(str(total))}}/{total}'\n",
        "        sys.stdout.write(f'\\r{progress}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMeGXS8azCy9"
      },
      "source": [
        "Ler arquivo de configuração de ambiente # https://dev.to/jakewitcher/using-env-files-for-environment-variables-in-python-applications-55a1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xDLz_3vzCy_"
      },
      "outputs": [],
      "source": [
        "def getEnv(env):\n",
        "    return os.getenv(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSLIiO4v6-aI",
        "outputId": "c35cac28-3d06-45ba-8b4f-2c0b03b212e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to .env_template\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a .env_template\n",
        "OUTPUT_FILES_PATH=./OUTPUT_FILES/\n",
        "EXTRACTED_FILES_PATH=./EXTRACTED_FILES/\n",
        "PATH_PARQUET=./PATH_PARQUET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3fgVU5hzCy_",
        "outputId": "d5bfa461-7450-491a-f2d8-b928115e6f9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print('Especifique o local do seu arquivo de configuração \".env\". Por exemplo: C:\\...\\Receita_Federal_do_Brasil_-_Dados_Publicos_CNPJ\\code')\n",
        "# C:\\Aphonso_C\\Git\\Receita_Federal_do_Brasil_-_Dados_Publicos_CNPJ\\code\n",
        "# local_env = input()\n",
        "local_env = \"./\"\n",
        "dotenv_path = os.path.join(local_env, '.env_template')\n",
        "load_dotenv(dotenv_path=dotenv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXtwjdlWzCzB"
      },
      "outputs": [],
      "source": [
        "dados_rf = 'http://200.152.38.155/CNPJ/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB2MYqEczCzC"
      },
      "source": [
        "Read details from \".env\" file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fESmsPp-zCzD",
        "outputId": "c8dcf78e-6e8b-4e97-e234-17dadd03384f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diretórios definidos: \n",
            "output_files: ./OUTPUT_FILES/\n",
            "parquet_files: ./PATH_PARQUET\n",
            "extracted_files: ./EXTRACTED_FILES/\n"
          ]
        }
      ],
      "source": [
        "output_files = None\n",
        "extracted_files = None\n",
        "parquet_files = None\n",
        "try:\n",
        "    output_files = getEnv('OUTPUT_FILES_PATH')\n",
        "    makedirs(output_files)\n",
        "    extracted_files = getEnv('EXTRACTED_FILES_PATH')\n",
        "    makedirs(extracted_files)\n",
        "\n",
        "    parquet_files = getEnv('PATH_PARQUET')\n",
        "    makedirs(parquet_files)\n",
        "\n",
        "    print('Diretórios definidos: \\n' +\n",
        "          'output_files: ' + str(output_files)  + '\\n' +\n",
        "          'parquet_files: ' + str(parquet_files)  + '\\n' +\n",
        "          'extracted_files: ' + str(extracted_files)\n",
        "          )\n",
        "except:\n",
        "    pass\n",
        "    print('Erro na definição dos diretórios, verifique o arquivo \".env\" ou o local informado do seu arquivo de configuração.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qopUzoXRzCzD"
      },
      "outputs": [],
      "source": [
        "# def to_parquet(dataframe, file_name):\n",
        "#     root = f\"./{parquet_files}/\"\n",
        "#     dataframe.to_parquet(f'{root}{file_name}.parquet', index=False, compression='snappy')\n",
        "\n",
        "def to_parquet(dataframe, file_name, append=True):\n",
        "    root = f\"./{parquet_files}/\"\n",
        "    file_path = f'{root}{file_name}.parquet'\n",
        "\n",
        "    if append and os.path.exists(file_path):\n",
        "        # Se o arquivo Parquet existir e a opção de append estiver ativada, acrescente os novos dados\n",
        "        existing_data = pd.read_parquet(file_path)\n",
        "        combined_data = pd.concat([existing_data, dataframe], ignore_index=True)\n",
        "        combined_data.to_parquet(file_path, index=False, compression='snappy',engine=\"pyarrow\")\n",
        "        print(f'Dados acrescentados a {file_path}')\n",
        "    else:\n",
        "        # Se o arquivo Parquet não existir ou a opção de append estiver desativada, crie um novo\n",
        "        dataframe.to_parquet(file_path, index=False, compression='snappy',engine=\"pyarrow\")\n",
        "        print(f'Novo arquivo Parquet criado em {file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM28kq-wAY2E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ribiqdPPAbiI"
      },
      "source": [
        "# Extração CNPJS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXbGlxpjzCzE"
      },
      "source": [
        "## 1. lendo urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4dOsXWMzCzF"
      },
      "outputs": [],
      "source": [
        "dados_rf = 'http://200.152.38.155/CNPJ/'\n",
        "raw_html = urllib.request.urlopen(dados_rf)\n",
        "raw_html = raw_html.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EqH7Fg6zCzF"
      },
      "source": [
        "Formatar página e converter em string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6teoco_tzCzF"
      },
      "outputs": [],
      "source": [
        "page_items = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "html_str = str(page_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtVz9s_XzCzG"
      },
      "source": [
        "## 2. Coletar arquivos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs1IhA8bzCzH"
      },
      "source": [
        "### 2.1 Obter arquivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QnhV3KLzCzJ"
      },
      "outputs": [],
      "source": [
        "Files = []\n",
        "text = '.zip'\n",
        "for m in re.finditer(text, html_str):\n",
        "    i_start = m.start()-40\n",
        "    i_end = m.end()\n",
        "    i_loc = html_str[i_start:i_end].find('href=')+6\n",
        "    Files.append(html_str[i_start+i_loc:i_end])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C5NNGeNzCzK"
      },
      "source": [
        "Correcao do nome dos arquivos devido a mudanca na estrutura do HTML da pagina - 31/07/22 - Aphonso Rafael"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4lj1y_5zCzK"
      },
      "outputs": [],
      "source": [
        "Files_clean = []\n",
        "for i in range(len(Files)):\n",
        "    if not Files[i].find('.zip\">') > -1:\n",
        "        Files_clean.append(Files[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjg-VLG4zCzL"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del Files\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7LT_QFxzCzL"
      },
      "outputs": [],
      "source": [
        "Files = Files_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yc4J-aVzCzL",
        "outputId": "36404ce4-5663-46ee-9915-4dd55750b122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arquivos que serão baixados:\n",
            "1 - Cnaes.zip\n",
            "2 - Empresas0.zip\n",
            "3 - Empresas1.zip\n",
            "4 - Empresas2.zip\n",
            "5 - Empresas3.zip\n",
            "6 - Empresas4.zip\n",
            "7 - Empresas5.zip\n",
            "8 - Empresas6.zip\n",
            "9 - Empresas7.zip\n",
            "10 - Empresas8.zip\n",
            "11 - Empresas9.zip\n",
            "12 - Estabelecimentos0.zip\n",
            "13 - Estabelecimentos1.zip\n",
            "14 - Estabelecimentos2.zip\n",
            "15 - Estabelecimentos3.zip\n",
            "16 - Estabelecimentos4.zip\n",
            "17 - Estabelecimentos5.zip\n",
            "18 - Estabelecimentos6.zip\n",
            "19 - Estabelecimentos7.zip\n",
            "20 - Estabelecimentos8.zip\n",
            "21 - Estabelecimentos9.zip\n",
            "22 - Motivos.zip\n",
            "23 - Municipios.zip\n",
            "24 - Naturezas.zip\n",
            "25 - Paises.zip\n",
            "26 - Qualificacoes.zip\n",
            "27 - Simples.zip\n",
            "28 - Socios0.zip\n",
            "29 - Socios1.zip\n",
            "30 - Socios2.zip\n",
            "31 - Socios3.zip\n",
            "32 - Socios4.zip\n",
            "33 - Socios5.zip\n",
            "34 - Socios6.zip\n",
            "35 - Socios7.zip\n",
            "36 - Socios8.zip\n",
            "37 - Socios9.zip\n"
          ]
        }
      ],
      "source": [
        "print('Arquivos que serão baixados:')\n",
        "i_f = 0\n",
        "for f in Files:\n",
        "    i_f += 1\n",
        "    print(str(i_f) + ' - ' + f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qesAqWEdzCzL"
      },
      "source": [
        "### 2.2 DOWNLOAD :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmpPJgz2zCzM"
      },
      "source": [
        "######################################################################################################################<br>\n",
        " DOWNLOAD ############################################################################################################<br>\n",
        "######################################################################################################################<br>\n",
        "Create this bar_progress method which is invoked automatically from wget:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-8BPkTOzCzR"
      },
      "outputs": [],
      "source": [
        "def bar_progress(current, total, width=80):\n",
        "  progress_message = \"Downloading: %d%% [%d / %d] bytes - \" % (current / total * 100, current, total)\n",
        "  # Don't use print() as it will print in new line every time.\n",
        "  sys.stdout.write(\"\\r\" + progress_message)\n",
        "  sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mQbbNInzCzV"
      },
      "source": [
        "Download arquivos ################################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBFbtBQ0zCzX",
        "outputId": "1fa085cf-a122-4f6b-89c4-6b873ea2d065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: 0% [3219456 / 345397675] bytes - "
          ]
        }
      ],
      "source": [
        "# @title Paralelizando download de arquivos\n",
        "# @markdown Download arquivos #####################\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def download_file(url, output_files):\n",
        "    nome_arquivo = os.path.join(output_files, os.path.basename(url))\n",
        "    if check_diff(url, nome_arquivo):\n",
        "        wget.download(url, out=output_files, bar=bar_progress)\n",
        "\n",
        "# Número de threads para usar no download paralelo\n",
        "num_threads = 4 # Ajuste conforme necessário\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    # Use o executor para enviar tarefas de download em paralelo\n",
        "    tarefas_download = [executor.submit(download_file, dados_rf + l, output_files) for l in Files]\n",
        "\n",
        "    # Aguarde a conclusão de todas as tarefas\n",
        "    for futuro in tarefas_download:\n",
        "        futuro.result()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Am1acxzCza"
      },
      "source": [
        "Download layout:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atBf7KDvzCzf"
      },
      "outputs": [],
      "source": [
        "Layout = 'https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/arquivos/NOVOLAYOUTDOSDADOSABERTOSDOCNPJ.pdf'\n",
        "print('Baixando layout:')\n",
        "wget.download(Layout, out=output_files, bar=bar_progress)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tbd6rTLzCzg"
      },
      "source": [
        "### 2.3 Extraindo Arquivos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0UJGrvizCzj"
      },
      "source": [
        "##################################################################################################################################################\n",
        "\n",
        "Extracting files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thqSBJdtzCzj"
      },
      "outputs": [],
      "source": [
        "# paralelizar extração de arquivos zip\n",
        "def extract_zip(file_path, extracted_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extracted_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao extrair {file_path}: {e}\")\n",
        "\n",
        "# Número de threads para usar na extração paralela\n",
        "num_threads = 8  # Ajuste conforme necessário\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    # Use o executor para enviar tarefas de extração em paralelo\n",
        "    tarefas_extracao = [executor.submit(extract_zip, os.path.join(output_files, l), extracted_files) for l in Files]\n",
        "\n",
        "    # Aguarde a conclusão de todas as tarefas\n",
        "    for futuro in tarefas_extracao:\n",
        "        futuro.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llyN7EDozCzl"
      },
      "source": [
        "## 3. Ler e inserir dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NKCAcUfzCzl"
      },
      "source": [
        "####################################################<br>\n",
        " LER E INSERIR DADOS ###############################<br>\n",
        "####################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQJD9O00zCzm"
      },
      "outputs": [],
      "source": [
        "insert_start = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kzddulJzCzm"
      },
      "source": [
        "Files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaOhFNQrzCzo"
      },
      "outputs": [],
      "source": [
        "Items = [name for name in os.listdir(extracted_files) if name.endswith('')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eXJInoUzCzq"
      },
      "source": [
        "Separar arquivos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z37Pfx8xzCzq"
      },
      "outputs": [],
      "source": [
        "arquivos_empresa = []\n",
        "arquivos_estabelecimento = []\n",
        "arquivos_socios = []\n",
        "arquivos_simples = []\n",
        "arquivos_cnae = []\n",
        "arquivos_moti = []\n",
        "arquivos_munic = []\n",
        "arquivos_natju = []\n",
        "arquivos_pais = []\n",
        "arquivos_quals = []\n",
        "for i in range(len(Items)):\n",
        "    if Items[i].find('EMPRE') > -1:\n",
        "        arquivos_empresa.append(Items[i])\n",
        "    elif Items[i].find('ESTABELE') > -1:\n",
        "        arquivos_estabelecimento.append(Items[i])\n",
        "    elif Items[i].find('SOCIO') > -1:\n",
        "        arquivos_socios.append(Items[i])\n",
        "    elif Items[i].find('SIMPLES') > -1:\n",
        "        arquivos_simples.append(Items[i])\n",
        "    elif Items[i].find('CNAE') > -1:\n",
        "        arquivos_cnae.append(Items[i])\n",
        "    elif Items[i].find('MOTI') > -1:\n",
        "        arquivos_moti.append(Items[i])\n",
        "    elif Items[i].find('MUNIC') > -1:\n",
        "        arquivos_munic.append(Items[i])\n",
        "    elif Items[i].find('NATJU') > -1:\n",
        "        arquivos_natju.append(Items[i])\n",
        "    elif Items[i].find('PAIS') > -1:\n",
        "        arquivos_pais.append(Items[i])\n",
        "    elif Items[i].find('QUALS') > -1:\n",
        "        arquivos_quals.append(Items[i])\n",
        "    else:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euHqiBjyzCzq"
      },
      "source": [
        "Conectar no banco de dados:<br>\n",
        "Dados da conexão com o BD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvDGG6LVzCzr"
      },
      "outputs": [],
      "source": [
        "# user=getEnv('DB_USER')\n",
        "# passw=getEnv('DB_PASSWORD')\n",
        "# host=getEnv('DB_HOST')\n",
        "# port=getEnv('DB_PORT')\n",
        "# database=getEnv('DB_NAME')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbLzwfVZzCzr"
      },
      "source": [
        "Conectar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE-AlB5SzCzr"
      },
      "outputs": [],
      "source": [
        "# conector para mysql\n",
        "# engine = create_engine(f'mysql+mysqlconnector://{user}:{passw}@{host}:{port}/{database}')\n",
        "# cur = conn.cursor()\n",
        "\n",
        "# engine = create_engine('postgresql://'+user+':'+passw+'@'+host+':'+port+'/'+database)\n",
        "# conn = psycopg2.connect('dbname='+database+' '+'user='+user+' '+'host='+host+' '+'port='+port+' '+'password='+passw)\n",
        "# cur = conn.cursor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptYi3gUHzCzr"
      },
      "source": [
        "### 3.1 Dados Empresas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ-VWL1SzCzr"
      },
      "outputs": [],
      "source": [
        "empresa_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "#######################\n",
        "## Arquivos de EMPRESA:\n",
        "#######################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8D5JCpczCzs"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V49CdFEzCzs"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"empresa\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egbsoIA0zCzs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# for e in range(0, len(arquivos_empresa)):\n",
        "for e in range(3, len(arquivos_empresa)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_empresa[e]+' [...]')\n",
        "    try:\n",
        "        del empresa\n",
        "    except:\n",
        "        pass\n",
        "    empresa = pd.DataFrame(columns=[0, 1, 2, 3, 4, 5, 6])\n",
        "    empresa_dtypes = {0: 'object', 1: 'object', 2: 'object', 3: 'object', 4: 'object', 5: 'object', 6: 'object'}\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_empresa[e])\n",
        "    empresa = pd.read_csv(filepath_or_buffer=extracted_file_path,\n",
        "                          sep=';',\n",
        "                          #nrows=100,\n",
        "                          skiprows=0,\n",
        "                          header=None,\n",
        "                          dtype=empresa_dtypes,\n",
        "                          encoding='latin-1'\n",
        "    )\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    empresa = empresa.reset_index()\n",
        "    del empresa['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    empresa.columns = ['cnpj_basico', 'razao_social', 'natureza_juridica', 'qualificacao_responsavel', 'capital_social', 'porte_empresa', 'ente_federativo_responsavel']\n",
        "\n",
        "    # Replace \",\" por \".\"\n",
        "    empresa['capital_social'] = empresa['capital_social'].apply(lambda x: x.replace(',','.'))\n",
        "    empresa['capital_social'] = empresa['capital_social'].astype(float)\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # Empresa\n",
        "\n",
        "    # chunksize = 100000\n",
        "    # for chunk in pd.read_csv(filepath_or_buffer=extracted_file_path,\n",
        "    #                       sep=';',\n",
        "    #                       #nrows=100,\n",
        "    #                       skiprows=0,\n",
        "    #                       header=None,\n",
        "    #                       dtype=empresa_dtypes,\n",
        "    #                       encoding='latin-1',\n",
        "    #                     #   low_memory=False,\n",
        "    #                       chunksize=chunksize\n",
        "    # ):\n",
        "    #      # Tratamento do arquivo antes de inserir na base:\n",
        "    #     empresa = empresa.reset_index()\n",
        "    #     del empresa['index']\n",
        "\n",
        "    #     # Renomear colunas\n",
        "    #     empresa.columns = ['cnpj_basico', 'razao_social', 'natureza_juridica', 'qualificacao_responsavel', 'capital_social', 'porte_empresa', 'ente_federativo_responsavel']\n",
        "\n",
        "    #     # Replace \",\" por \".\"\n",
        "    #     empresa['capital_social'] = empresa['capital_social'].apply(lambda x: x.replace(',','.'))\n",
        "    #     empresa['capital_social'] = empresa['capital_social'].astype(float)\n",
        "    #     to_parquet(empresa,'empresa')\n",
        "    to_parquet(empresa,'empresa')\n",
        "\n",
        "    # to_sql(empresa, name='empresa', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_empresa[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI-wttUTzCzt"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del empresa\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de empresa finalizados!')\n",
        "empresa_insert_end = time.time()\n",
        "empresa_Tempo_insert = round((empresa_insert_end - empresa_insert_start))\n",
        "print('Tempo de execução do processo de empresa (em segundos): ' + str(empresa_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-lmykfazCzt"
      },
      "source": [
        "### 3.2 Arquivos de estabelecimento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QfpcnTOzCzu"
      },
      "outputs": [],
      "source": [
        "estabelecimento_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "###############################\n",
        "## Arquivos de ESTABELECIMENTO:\n",
        "###############################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQCtsjr1zCzv"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaBQGYuGzCzv"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"estabelecimento\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTTjeeixzCzv"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_estabelecimento)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_estabelecimento[e]+' [...]')\n",
        "    try:\n",
        "        del estabelecimento\n",
        "    except:\n",
        "        pass\n",
        "    estabelecimento = pd.DataFrame(columns=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28])\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_estabelecimento[e])\n",
        "    estabelecimento = pd.read_csv(filepath_or_buffer=extracted_file_path,\n",
        "                          sep=';',\n",
        "                          #nrows=100,\n",
        "                          skiprows=0,\n",
        "                          header=None,\n",
        "                          dtype='object',\n",
        "                          encoding='latin-1',\n",
        "    )\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    estabelecimento = estabelecimento.reset_index()\n",
        "    del estabelecimento['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    estabelecimento.columns = ['cnpj_basico',\n",
        "                               'cnpj_ordem',\n",
        "                               'cnpj_dv',\n",
        "                               'identificador_matriz_filial',\n",
        "                               'nome_fantasia',\n",
        "                               'situacao_cadastral',\n",
        "                               'data_situacao_cadastral',\n",
        "                               'motivo_situacao_cadastral',\n",
        "                               'nome_cidade_exterior',\n",
        "                               'pais',\n",
        "                               'data_inicio_atividade',\n",
        "                               'cnae_fiscal_principal',\n",
        "                               'cnae_fiscal_secundaria',\n",
        "                               'tipo_logradouro',\n",
        "                               'logradouro',\n",
        "                               'numero',\n",
        "                               'complemento',\n",
        "                               'bairro',\n",
        "                               'cep',\n",
        "                               'uf',\n",
        "                               'municipio',\n",
        "                               'ddd_1',\n",
        "                               'telefone_1',\n",
        "                               'ddd_2',\n",
        "                               'telefone_2',\n",
        "                               'ddd_fax',\n",
        "                               'fax',\n",
        "                               'correio_eletronico',\n",
        "                               'situacao_especial',\n",
        "                               'data_situacao_especial']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # estabelecimento\n",
        "    to_parquet(estabelecimento,'estabelecimento')\n",
        "    # to_sql(estabelecimento, name='estabelecimento', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_estabelecimento[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erf0YMuYzCzw"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del estabelecimento\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de estabelecimento finalizados!')\n",
        "estabelecimento_insert_end = time.time()\n",
        "estabelecimento_Tempo_insert = round((estabelecimento_insert_end - estabelecimento_insert_start))\n",
        "print('Tempo de execução do processo de estabelecimento (em segundos): ' + str(estabelecimento_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmYX8j-QzCzx"
      },
      "source": [
        "### 3.3 Arquivos de socios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhgAT10XzCzy"
      },
      "outputs": [],
      "source": [
        "socios_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "######################\n",
        "## Arquivos de SOCIOS:\n",
        "######################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa7cFshbzCzz"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq5smip8zCzz"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"socios\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh9AvSj5zCz0"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_socios)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_socios[e]+' [...]')\n",
        "    try:\n",
        "        del socios\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_socios[e])\n",
        "    socios = pd.DataFrame(columns=[1,2,3,4,5,6,7,8,9,10,11])\n",
        "    socios = pd.read_csv(filepath_or_buffer=extracted_file_path,\n",
        "                          sep=';',\n",
        "                          #nrows=100,\n",
        "                          skiprows=0,\n",
        "                          header=None,\n",
        "                          dtype='object',\n",
        "                          encoding='latin-1',\n",
        "    )\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    socios = socios.reset_index()\n",
        "    del socios['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    socios.columns = ['cnpj_basico',\n",
        "                      'identificador_socio',\n",
        "                      'nome_socio_razao_social',\n",
        "                      'cpf_cnpj_socio',\n",
        "                      'qualificacao_socio',\n",
        "                      'data_entrada_sociedade',\n",
        "                      'pais',\n",
        "                      'representante_legal',\n",
        "                      'nome_do_representante',\n",
        "                      'qualificacao_representante_legal',\n",
        "                      'faixa_etaria']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # socios\n",
        "    to_parquet(socios,'socios')\n",
        "\n",
        "    # to_sql(socios, name='socios', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_socios[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOgEWtorzCz1"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del socios\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de socios finalizados!')\n",
        "socios_insert_end = time.time()\n",
        "socios_Tempo_insert = round((socios_insert_end - socios_insert_start))\n",
        "print('Tempo de execução do processo de sócios (em segundos): ' + str(socios_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chcxuZlLzCz2"
      },
      "source": [
        "### 3.4 Arquivos de simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ_R1MlgzCz2"
      },
      "outputs": [],
      "source": [
        "simples_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "################################\n",
        "## Arquivos do SIMPLES NACIONAL:\n",
        "################################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieK2kQtUzCz3"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h_MOv0TzCz3"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"simples\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLC-8RjszCz4"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_simples)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_simples[e]+' [...]')\n",
        "    try:\n",
        "        del simples\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Verificar tamanho do arquivo:\n",
        "    print('Lendo o arquivo ' + arquivos_simples[e]+' [...]')\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_simples[e])\n",
        "    simples_lenght = sum(1 for line in open(extracted_file_path, \"r\"))\n",
        "    print('Linhas no arquivo do Simples '+ arquivos_simples[e] +': '+str(simples_lenght))\n",
        "    tamanho_das_partes = 1000000 # Registros por carga\n",
        "    partes = round(simples_lenght / tamanho_das_partes)\n",
        "    nrows = tamanho_das_partes\n",
        "    skiprows = 0\n",
        "    print('Este arquivo será dividido em ' + str(partes) + ' partes para inserção no banco de dados')\n",
        "    for i in range(0, partes):\n",
        "        print('Iniciando a parte ' + str(i+1) + ' [...]')\n",
        "        simples = pd.DataFrame(columns=[1,2,3,4,5,6])\n",
        "        simples = pd.read_csv(filepath_or_buffer=extracted_file_path,\n",
        "                              sep=';',\n",
        "                              nrows=nrows,\n",
        "                              skiprows=skiprows,\n",
        "                              header=None,\n",
        "                              dtype='object',\n",
        "                              encoding='latin-1',\n",
        "        )\n",
        "\n",
        "        # Tratamento do arquivo antes de inserir na base:\n",
        "        simples = simples.reset_index()\n",
        "        del simples['index']\n",
        "\n",
        "        # Renomear colunas\n",
        "        simples.columns = ['cnpj_basico',\n",
        "                           'opcao_pelo_simples',\n",
        "                           'data_opcao_simples',\n",
        "                           'data_exclusao_simples',\n",
        "                           'opcao_mei',\n",
        "                           'data_opcao_mei',\n",
        "                           'data_exclusao_mei']\n",
        "        skiprows = skiprows+nrows\n",
        "\n",
        "        # Gravar dados no banco:\n",
        "        # simples\n",
        "        to_parquet(simples,'simples')\n",
        "        # to_sql(simples, name='simples', con=engine, if_exists='append', index=False)\n",
        "        print('Arquivo ' + arquivos_simples[e] + ' inserido com sucesso no banco de dados! - Parte '+ str(i+1))\n",
        "        try:\n",
        "            del simples\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QMGGV7azCz4"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del simples\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujy5Rkh8zCz5"
      },
      "outputs": [],
      "source": [
        "print('Arquivos do simples finalizados!')\n",
        "simples_insert_end = time.time()\n",
        "simples_Tempo_insert = round((simples_insert_end - simples_insert_start))\n",
        "print('Tempo de execução do processo do Simples Nacional (em segundos): ' + str(simples_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIAC1m6-zCz6"
      },
      "source": [
        "### 3.5 Arquivos de cnae:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX97QRbazCz7"
      },
      "outputs": [],
      "source": [
        "cnae_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "######################\n",
        "## Arquivos de cnae:\n",
        "######################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0VJxlJvzCz7"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49MHnv8mzC0C"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"cnae\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-Ly9cDmzC0D"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_cnae)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_cnae[e]+' [...]')\n",
        "    try:\n",
        "        del cnae\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_cnae[e])\n",
        "    cnae = pd.DataFrame(columns=[1,2])\n",
        "    cnae = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    cnae = cnae.reset_index()\n",
        "    del cnae['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    cnae.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # cnae\n",
        "    to_parquet(cnae,'cnae')\n",
        "\n",
        "    # to_sql(cnae, name='cnae', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_cnae[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9uaCJBVzC0E"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del cnae\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de cnae finalizados!')\n",
        "cnae_insert_end = time.time()\n",
        "cnae_Tempo_insert = round((cnae_insert_end - cnae_insert_start))\n",
        "print('Tempo de execução do processo de cnae (em segundos): ' + str(cnae_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIU70T3uzC0J"
      },
      "source": [
        "### 3.6 Arquivos de moti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN19b753zC0J"
      },
      "outputs": [],
      "source": [
        "moti_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "#########################################\n",
        "## Arquivos de motivos da situação atual:\n",
        "#########################################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG9sVUqMzC0M"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKXmYPShzC0N"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"moti\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_TuuMbDzC0O"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_moti)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_moti[e]+' [...]')\n",
        "    try:\n",
        "        del moti\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_moti[e])\n",
        "    moti = pd.DataFrame(columns=[1,2])\n",
        "    moti = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    moti = moti.reset_index()\n",
        "    del moti['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    moti.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # moti\n",
        "    to_parquet(moti,'moti')\n",
        "\n",
        "    # to_sql(moti, name='moti', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_moti[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCxTN-qTzC0O"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del moti\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de moti finalizados!')\n",
        "moti_insert_end = time.time()\n",
        "moti_Tempo_insert = round((moti_insert_end - moti_insert_start))\n",
        "print('Tempo de execução do processo de motivos da situação atual (em segundos): ' + str(moti_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vol2W8L1zC0P"
      },
      "source": [
        "### 3.7 Arquivos de munic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J059ILr8zC0P"
      },
      "outputs": [],
      "source": [
        "munic_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "##########################\n",
        "## Arquivos de municípios:\n",
        "##########################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B0U052NzC0R"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph8IEFUxzC0S"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"munic\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-7UoNfbzC0T"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_munic)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_munic[e]+' [...]')\n",
        "    try:\n",
        "        del munic\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_munic[e])\n",
        "    munic = pd.DataFrame(columns=[1,2])\n",
        "    munic = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    munic = munic.reset_index()\n",
        "    del munic['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    munic.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # munic\n",
        "    to_parquet(munic,'munic')\n",
        "\n",
        "    # to_sql(munic, name='munic', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_munic[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofn2-16DzC0U"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del munic\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de munic finalizados!')\n",
        "munic_insert_end = time.time()\n",
        "munic_Tempo_insert = round((munic_insert_end - munic_insert_start))\n",
        "print('Tempo de execução do processo de municípios (em segundos): ' + str(munic_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZrM1eYzC0V"
      },
      "source": [
        "### 3.8 Arquivos de natureza juridica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gRVF9_OzC0V"
      },
      "outputs": [],
      "source": [
        "natju_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "#################################\n",
        "## Arquivos de natureza jurídica:\n",
        "#################################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgndbOdmzC0W"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QljUxgN4zC0X"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"natju\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr44EPT4zC0X"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_natju)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_natju[e]+' [...]')\n",
        "    try:\n",
        "        del natju\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_natju[e])\n",
        "    natju = pd.DataFrame(columns=[1,2])\n",
        "    natju = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    natju = natju.reset_index()\n",
        "    del natju['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    natju.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # natju\n",
        "    to_parquet(natju,'natju')\n",
        "\n",
        "    # to_sql(natju, name='natju', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_natju[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtXQ7RsUzC0X"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del natju\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de natju finalizados!')\n",
        "natju_insert_end = time.time()\n",
        "natju_Tempo_insert = round((natju_insert_end - natju_insert_start))\n",
        "print('Tempo de execução do processo de natureza jurídica (em segundos): ' + str(natju_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT8kf1-SzC0X"
      },
      "source": [
        "### 3.9 Arquivos de pais:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAx5ua94zC0Y"
      },
      "outputs": [],
      "source": [
        "pais_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "######################\n",
        "## Arquivos de país:\n",
        "######################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMYOt-0YzC0Y"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztRlD5sUzC0Y"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"pais\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWcdyX4yzC0Z"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_pais)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_pais[e]+' [...]')\n",
        "    try:\n",
        "        del pais\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_pais[e])\n",
        "    pais = pd.DataFrame(columns=[1,2])\n",
        "    pais = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    pais = pais.reset_index()\n",
        "    del pais['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    pais.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # pais\n",
        "    to_parquet(pais,'pais')\n",
        "\n",
        "    # to_sql(pais, name='pais', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_pais[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlz9RyIdzC0Z"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del pais\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de pais finalizados!')\n",
        "pais_insert_end = time.time()\n",
        "pais_Tempo_insert = round((pais_insert_end - pais_insert_start))\n",
        "print('Tempo de execução do processo de país (em segundos): ' + str(pais_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDwXRW7HzC0Z"
      },
      "source": [
        "### 3.10 Arquivos de qualificação de sócios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64HvIx22zC0a"
      },
      "outputs": [],
      "source": [
        "quals_insert_start = time.time()\n",
        "print(\"\"\"\n",
        "######################################\n",
        "## Arquivos de qualificação de sócios:\n",
        "######################################\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBnT8XUYzC0b"
      },
      "source": [
        "Drop table antes do insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLNjpbLYzC0c"
      },
      "outputs": [],
      "source": [
        "# cur.execute('DROP TABLE IF EXISTS \"quals\";')\n",
        "# conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZn98zoGzC0f"
      },
      "outputs": [],
      "source": [
        "for e in range(0, len(arquivos_quals)):\n",
        "    print('Trabalhando no arquivo: '+arquivos_quals[e]+' [...]')\n",
        "    try:\n",
        "        del quals\n",
        "    except:\n",
        "        pass\n",
        "    extracted_file_path = os.path.join(extracted_files, arquivos_quals[e])\n",
        "    quals = pd.DataFrame(columns=[1,2])\n",
        "    quals = pd.read_csv(filepath_or_buffer=extracted_file_path, sep=';', skiprows=0, header=None, dtype='object', encoding='ANSI')\n",
        "\n",
        "    # Tratamento do arquivo antes de inserir na base:\n",
        "    quals = quals.reset_index()\n",
        "    del quals['index']\n",
        "\n",
        "    # Renomear colunas\n",
        "    quals.columns = ['codigo', 'descricao']\n",
        "\n",
        "    # Gravar dados no banco:\n",
        "    # quals\n",
        "    to_parquet(quals,'quals')\n",
        "\n",
        "    # to_sql(quals, name='quals', con=engine, if_exists='append', index=False)\n",
        "    print('Arquivo ' + arquivos_quals[e] + ' inserido com sucesso no banco de dados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc5dAuThzC0h"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del quals\n",
        "except:\n",
        "    pass\n",
        "print('Arquivos de quals finalizados!')\n",
        "quals_insert_end = time.time()\n",
        "quals_Tempo_insert = round((quals_insert_end - quals_insert_start))\n",
        "print('Tempo de execução do processo de qualificação de sócios (em segundos): ' + str(quals_Tempo_insert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAbpxQeszC0i"
      },
      "source": [
        "%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfo4tOfMzC0i"
      },
      "outputs": [],
      "source": [
        "insert_end = time.time()\n",
        "Tempo_insert = round((insert_end - insert_start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S64tHTw6zC0j"
      },
      "source": [
        "# Finalizando Extração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDzzgXUezC0k"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "#############################################<br>\n",
        "## Processo de carga dos arquivos finalizado!<br>\n",
        "#############################################<br>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtyB1fOQzC0k"
      },
      "outputs": [],
      "source": [
        "print('Tempo total de execução do processo de carga (em segundos): ' + str(Tempo_insert)) # Tempo de execução do processo (em segundos): 17.770 (4hrs e 57 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmkQjnZWzC0l"
      },
      "source": [
        "###############################<br>\n",
        "Tamanho dos arquivos:<br>\n",
        "empresa = 45.811.638<br>\n",
        "estabelecimento = 48.421.619<br>\n",
        "socios = 20.426.417<br>\n",
        "simples = 27.893.923<br>\n",
        "###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlE3c2OozC0l"
      },
      "source": [
        "Criar índices na base de dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HznxdePBzC0l"
      },
      "outputs": [],
      "source": [
        "index_start = time.time()\n",
        "print(\"\"\"\n",
        "#######################################\n",
        "## Criar índices na base de dados [...]\n",
        "#######################################\n",
        "\"\"\")\n",
        "# cur.execute(\"\"\"\n",
        "# create index empresa_cnpj on empresa(cnpj_basico);\n",
        "# commit;\n",
        "# create index estabelecimento_cnpj on estabelecimento(cnpj_basico);\n",
        "# commit;\n",
        "# create index socios_cnpj on socios(cnpj_basico);\n",
        "# commit;\n",
        "# create index simples_cnpj on simples(cnpj_basico);\n",
        "# commit;\n",
        "# \"\"\")\n",
        "# conn.commit()\n",
        "print(\"\"\"\n",
        "############################################################\n",
        "## Índices criados nas tabelas, para a coluna `cnpj_basico`:\n",
        "   - empresa\n",
        "   - estabelecimento\n",
        "   - socios\n",
        "   - simples\n",
        "############################################################\n",
        "\"\"\")\n",
        "index_end = time.time()\n",
        "index_time = round(index_end - index_start)\n",
        "print('Tempo para criar os índices (em segundos): ' + str(index_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rijPu3jFzC0m"
      },
      "source": [
        "%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCx2ON81zC0m"
      },
      "source": [
        "print(\n",
        "Processo 100% finalizado! Você já pode usar seus dados no BD!<br>\n",
        " - Desenvolvido por: Aphonso Henrique do Amaral Rafael<br>\n",
        " - Contribua com esse projeto aqui: https://github.com/aphonsoar/Receita_Federal_do_Brasil_-_Dados_Publicos_CNPJ<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuzOFq2BBN-O"
      },
      "source": [
        "# Tratamento\n",
        "\n",
        "apartir da consolidação dos dados de cnpj, iremos montar interações entre as bases parquet com duckdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2BOBAhKHBQXn"
      },
      "outputs": [],
      "source": [
        "# @title lendo dados direto de arquivos parquet\n",
        "\n",
        "import duckdb\n",
        "duckdb.sql(\"\"\"\n",
        "  SELECT *\n",
        "  FROM read_parquet('input.parquet')\n",
        "  \"\"\"\n",
        "  );\n",
        "\n",
        "print(duckdb.query('''\n",
        "  SELECT COUNT(*)\n",
        "  FROM 'taxi_2019_04.parquet'\n",
        "  WHERE pickup_at BETWEEN '2019-04-15' AND '2019-04-20'\n",
        "  ''').fetchall())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jLgF3Ha15D2l",
        "RFXLnHkR5IVw",
        "EXbGlxpjzCzE",
        "Fs1IhA8bzCzH",
        "1tbd6rTLzCzg",
        "llyN7EDozCzl",
        "I5ZrM1eYzC0V",
        "lT8kf1-SzC0X",
        "TDwXRW7HzC0Z",
        "S64tHTw6zC0j"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
